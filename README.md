ğŸ§  Multi-LLM Router

Cost-efficient, fault-tolerant multi-LLM routing system built with FastAPI.
Routes prompts intelligently to multiple LLM providers (Groq & OpenAI) with automatic fallback, token-based cost tracking, and a real-time monitoring dashboard.

ğŸ“Œ Repo: https://github.com/kaifnesstutorials/multi-llm-router

ğŸ“Œ Overview

Multi-LLM Router is a scalable microservice that intelligently sends user prompts to multiple Large Language Model (LLM) providers based on cost, availability, and reliability.

It provides:

Automatic failover between providers

Token usage and cost tracking per request

Monitoring dashboard for provider health, latency, and usage

This is ideal for developers looking to optimize LLM usage, reduce costs, and maintain reliability.

ğŸ§  Why This Project?

Direct usage of LLM APIs can be expensive and unreliable:

Some providers are cheaper but may fail occasionally

High token usage increases cost

Limited observability into latency, usage, or provider success

Multi-LLM Router solves these challenges by:

Prioritizing cheapest providers first

Falling back automatically if a provider fails

Logging token usage and cost

Providing observability via a dashboard

ğŸ’¡ Key Features
ğŸ§  Smart Routing

Dynamically sorts providers by cost

Routes to the cheapest available provider

Automatic fallback if a provider fails

ğŸ’° Cost Optimization

Token counting and cost calculation per request

Optional prompt optimization to reduce token usage

Aggregated cost tracking in dashboard

ğŸ©º Reliability

Provider health monitoring (success/failure)

Latency tracking per provider

Success rate percentage tracking

ğŸ“Š Observability & Dashboard

Usage statistics (tokens, cost, requests)

Provider usage bar charts

Request history and prompt optimization warnings

âš¡ Prompt Optimization

Trims overly large prompts to reduce token cost

Displays warning in dashboard when a prompt is trimmed

ğŸ“‚ Project Structure
multi-llm-router/
â”‚
â”œâ”€â”€ app.py                    # FastAPI main application
â”œâ”€â”€ router.py                 # Multi-LLM routing logic
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ logger.py             # Usage logging
â”œâ”€â”€ providers/
â”‚   â”œâ”€â”€ base.py               # Provider base class
â”‚   â”œâ”€â”€ groq_provider.py      # Groq API integration
â”‚   â””â”€â”€ openai_provider.py    # OpenAI API integration
â”œâ”€â”€ config/
â”‚   â””â”€â”€ providers.yaml        # Provider configuration
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html            # Frontend HTML dashboard
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ usage.json            # Stored usage logs
â”œâ”€â”€ requirements.txt          # Python dependencies
â””â”€â”€ .gitignore

ğŸš€ Setup & Installation

1ï¸âƒ£ Clone the Repository

git clone https://github.com/kaifnesstutorials/multi-llm-router.git
cd multi-llm-router


2ï¸âƒ£ Create a Virtual Environment

python -m venv venv
# Windows
venv\Scripts\activate
# Linux/macOS
source venv/bin/activate


3ï¸âƒ£ Install Dependencies

pip install -r requirements.txt


4ï¸âƒ£ Update providers.yaml with your API keys

providers:
  - name: groq
    base_url: https://api.groq.com/openai/v1
    model: llama-3.1-8b-instant
    api_key: "YOUR_GROQ_KEY"
    cost_per_1k_tokens: 0.0005
    timeout: 30

  - name: openai
    base_url: https://api.openai.com/v1
    model: gpt-4o-mini
    api_key: "YOUR_OPENAI_KEY"
    cost_per_1k_tokens: 0.003
    timeout: 30


âš ï¸ Do not commit API keys to GitHub.

5ï¸âƒ£ Run the Server

uvicorn app:app --reload


Open the dashboard at: http://127.0.0.1:8000

ğŸ“Œ API Endpoints
ğŸ”¹ POST /generate

Generate a response using the routing system.

Request Body:

{
  "prompt": "Explain AI in simple words"
}


Response Example:

{
  "provider": "openai",
  "modelUsed": "gpt-4o-mini",
  "tokens": 15,
  "cost": 0.000045,
  "latency": 1.2,
  "response": "AI stands for Artificial Intelligence...",
  "status": "success"
}

ğŸ”¹ GET /stats

Returns usage statistics:

Total tokens used

Total cost

Request counts

ğŸ§‘â€ğŸ’» Author

Mohd Kaif Shaikh â€“ AI & Data Science Enthusiast
